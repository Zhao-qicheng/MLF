# 数据与任务相关
data/custom: 数据集来源类型（此处为自定义数据接口）
root_path: 数据根目录
data_path: 数据文件名（如 weather.csv）
data_type/data_real: 数据集名称/标识（如 weather）
{features: 'M'}: 输入特征类型。常见设定：M（Multivariate Input，Multivariate Output）（多变量输入、多变量输出），S(Univariate Input)（单变量输入），MS(Mutivariate Input，Univariate Output)（多变量输入、单变量输出）
target: 目标列名（如 OT）
{freq: 'h'}: 采样频率（如 h 表示小时）

# 输入/输出窗口与通道
{seq_len: 120}: 编码器输入的时间序列长度（历史窗口，即模型用于学习和推理的历史数据量）,表示编码器将输入过去的120个时间步的数据作为输入进行训练或推理
{label_len: 0}: 解码器标签长度（常用于 seq2seq 的 warm-up；0 表示不使用）,即先使用已知的标签序列来引导模型学习，然后再使用模型生成的预测进行下一步的训练
{pred_len: 1}: 预测步数（预测窗口），即模型预测未来的时间步数，决定了解码器的输出长度
{enc_in: 7}: 编码器输入通道数（输入变量个数，或者说是输入特征的数量）
{dec_in: 7}: 解码器输入通道数（解码侧使用的变量数，常与输出对齐）
{c_out: 7}: 最终输出通道数（预测的变量数）

# 模型结构（Transformer/时序骨干）
{d_model: 128}: 特征嵌入维度（Transformer 宽度）,输入数据会嵌入层转换为d_model维的向量，然后进入后续的编码器和解码器
{n_heads: 4}: 多头注意力头数，根据输入序列的不同部分计算加权和，使得模型能够关注输入中的不同部分
{e_layers: 4}: 编码器层数，transfomer模型的编码器部分由多个层组成，每一层包含自注意力机制和前馈神经网络。每一层都对输入序列进行处理并生成上下文相关的表示
{d_layers: 1}: 解码器层数，与编码器结构相似，由多个自注意力和前馈神经网络组成。根据编码器生成的表示来生成输出序列
{d_ff: 128}: 前馈网络隐藏维度（Transformer FFN）,它由两个线性层组成，中间通过非线性激活函数连接
embed: 时间特征嵌入方式（如 timeF）
{activation: True}: 激活函数（如 relu）,引入学习非线性关系
dropout: 模型通用 dropout 比例
{head_dropout: 0}: 输出头/特定头部 dropout,这里指头部不使用dropout
{fc_dropout: 0.2}: 全连接层/投影层的 dropout，这里表示在全连接层随机丢失20%的神经元以防止过拟合
output_attention: 是否输出注意力权重
{distil: True}: 是否启用编码器蒸馏（Informer 风格的下采样加速）,有两种策略，分别为下采样和蒸馏
Patch/分块与多尺度（MLF 特有）
{patch_len: 16}: 时间块（patch）的长度，这里指一个patch有16个时间步
{stride: 8}: 相邻 patch 的步长，这里指每隔8个时间步取一个patch
{padding_patch: "end"}: patch 末端的填充策略（如 end），这里指在不够一个patch的末尾添加end字符串进行补齐
fixed_patch_num: 固定每序列的 patch 数（启用时对 patch 数目做约束）
patch_pad: 是否对不满一个 patch 的尾部进行填充
max_patch_len: 允许的最大 patch 长度
patch_squeeze: 是否对 patch 表征做通道压缩（squeeze）
squeeze_factor: 压缩倍率（可为数组，对应多尺度的各层）
equal_patch_len: 各尺度使用的规范化 patch 长度列表
{scal_all: [5,10,30,120]}: 各尺度对应的时间尺度或窗口（如 [128, 512, 1024, 2048]）,通常在多尺度建模中使用，表示不同尺度下处理数据是的时间窗口大小，尤其适用于具有多层次变化的任务
patchLen_stride_all: 各尺度对应的 [patch_len, stride] 配置
shared_num: 多尺度/多模块间共享参数的组数
threshold_patch_num: 与动态选择/稀疏化相关的 patch 数门限
{use_multi_scale: False}: 是否启用多尺度分块/建模,指在多个尺度或分辨率上处理数据，适用于具有长期和短期变化的时间序列数据，多尺度建模可以帮助模型捕捉不同时间跨度的变化特征，这里是不启用
{scales: [16,8,4,2,1]}: 这是一个尺度列表(较大尺度展示长期的趋势，较小尺度捕捉短期波动)，下采样是降低数据的分辨率或减少数据的样本数
{scale_factor: 2}: 是一个比例因子，用于决定下采样的程度，这里是指将数据缩小到原来的1/2
MAP: 是否启用 MLF 中的 MAP 模块（模型特定组件）
MAP_alpha: MAP 模块的损失/权重/温度等超参（具体作用依实现）
redundancy_scaling: 是否对冗余特征进行缩放/抑制
activation_tag: 是否在特定阶段启用激活标记（模型内控制开关）
embed_dim: 特殊嵌入/时间嵌入维度（与 d_model 不同，常用于时间或标签嵌入）

# 归一化/预处理与分解
{revin/revin_norm: 1}: 是否启用 RevIN（可逆实例归一化），1表示使用，0表示不使用，归一化即减去均值并除以方差，可逆实例归一化则增加了一些参数，可以通过特定的操作恢复原始数据,从而保留了更多的输入细节
{affine: 0}: RevIN 的仿射参数开关（是否学习缩放/平移），用于对归一化的数据进行线性变换，从而适应特定的数据模型。一般由两部分参数组成，一个为缩放Scale，一个为平移Shift。常用0表示不使用，1表示使用
{subtract_last: 0}: 是否在归一化时减去序列的末值（对缓慢漂移和长期趋势的序列有用），适用于减少长期缓慢变化的趋势，使模型更加关注于有意义的短期变化。常用1表示使用，0表示不做任何修改
{decomposition: 0}: 是否做趋势-季节分解，将时间序列分解成主要成分：趋势（长期平稳的变化模式，如上升或者下降），季节性（周期性变化，如年复一年的季节性波动）和残差（剩余的不可预测部分，如随机波动或噪声）。常用1表示使用，0表示不使用
{moving_avg/kernel_size: 25}: 分解中的均值核大小（平滑窗口），值大关注长期趋势，值小关注短期波动
{individual: 0}: 通道(如图像的红、绿、蓝三个通道)是否独立建模/独立参数，这里指所有通道共享一组参数，适用于通道之间特征较为一致或者期望减少模型复杂度的情况
D_norm: 其他归一化开关（模型内自定义）

# 训练与优化
{is_training/state: True}: 训练状态（train/test）,这里表示现在处于训练状态
train_only: 仅训练（不做验证/测试）
train_epochs: 训练轮数
batch_size: 批大小
learning_rate: 初始学习率
lradj: 学习率调整策略（如 type1）
{patience: 100}: 早停耐心值,在验证集上没有明显提升的情况下继续训练的最大轮数(即允许最多连续多少轮没有提升)
loss: 损失函数（如 mse）
use_amp: 是否使用混合精度（AMP）
{num_workers: 10}: DataLoader 线程数，控制了数据加载的并行度，制定了在加载数据时同时运行多少个子进程（线程），来加速数据加载过程
itr: 重复实验次数（不同随机种子或多次运行）
do_predict: 仅推理预测模式（不训练）
{reconstruct_loss: False}: 是否加入重构损失（辅助任务），目标是通过模型生成的输出与原始输入之间的差异来进行优化，常用于需要重建原始输入数据的模型中，如自编码器或者某些生成对抗网络
{prob_forecasting: False}: 是否做概率预测（分布输出），即模型不仅会给出某个结果的预测值，还会输出每个类别的概率，输出一个可能值的概率分布

# 设备与加速
use_gpu/gpu/devices: GPU 使用与设备编号
use_multi_gpu: 是否多卡训练
device: 实际设备标识（如 cuda:0）
speed_mode: 加速模式开关（可能启用更激进的优化）

# 运行/记录与标识
checkpoints: 模型权重与日志存放路径
des: 实验描述
record: 是否记录训练曲线/日志
model/model_id: 模型名称与实验编号
mode: 任务模式（如 long-term 长期预测）
data_type: 数据类型标签（如 weather）
script_id: 脚本/实验运行标识
only_test: 是否只测试（加载已训练权重）
context_window: 上下文窗口（None 表示按默认/禁用）
explore_fund_memory: 与“记忆/外部存储”相关的探索性开关（模型特性）

# 常用关键项小结（优先调的超参）
序列窗口：seq_len、pred_len
通道维度：enc_in、dec_in、c_out
模型规模：d_model、n_heads、e_layers、d_layers、d_ff、dropout
Patch/多尺度：patch_len、stride、patchLen_stride_all、scal_all、max_patch_len
训练：learning_rate、batch_size、train_epochs、lradj、patience
归一化/预处理：revin、affine、moving_avg/kernel_size、decomposition